## Part B: Downloading genomics data quickly (using AWS instances)

1. Data description

We will download several types of data:
- a human reference genome
- a set of Illumina reads

To illustrate that download speeds are really fast on AWS, we chose to create our instance on the us-west-2 region, which happens to be the same region where the T2T human genome is hosted (https://github.com/marbl/CHM13).

So, specifically, we will retrieve:
- the CHM13v2 genome assembly v1.1
- Illumina PCR-free reads generated by T2T from CHM13

Why Illumina reads you ask? It's become, for some reason, T2T didn't release the HiFi reads on their S3 folder directly (they only release the raw PacBio subreads that need further processing). For the purpose of keeping this introduction relatively straightforward, we will download directly from S3 instead of from the SRA (which we will attempt at the very end of this document).

Why CHM13v1.1 and not v2.0? Because I had other plans in an earlier version of this document. But it shouldn't make a difference here, and you should feel free to download the v2.0 if you wish.

2. S3

S3 is the name of the storage service/protocol used by Amazon. Amazon uses a lot of terminology for its services. A S3 'repository' is called a bucket. 

You can think of a S3 bucket as someone's Dropbox with a special command-line tool to access it. Some buckets are public, some are private, and you can imagine in both cases that permissions are set so that buckets can be either read-only or read-write by everyone. Of course, the T2T buckets are read-only for us. 

A particular aspect of S3 is that you may sometimes need a AWS account to download data if it is non-public. Yet, by default the S3 command-line tool always wants you to sign in to your account. We'll avoid that by passing the ```--no-sign-request``` argument.

3. Downloading the data

We'll get the data from https://github.com/marbl/CHM13.

Go to your instance and create a folder.

    mkdir data && cd data

Check that you have enough space:

    df -h .
    
Which should print:

    Filesystem      Size  Used Avail Use% Mounted on
    /dev/nvme0n1p1   30G  1,6G   29G   6% /

Download CHM13 v1.1:

    aws s3 --no-sign-request cp s3://human-pangenomics/T2T/CHM13/assemblies/chm13.draft_v1.1.fasta.gz .

Download (a portion of) PCRfree reads:

    aws s3 --no-sign-request cp s3://human-pangenomics/T2T/CHM13/10x/CHM13_prep5_S13_L002_R1_001.fastq.gz .
    aws s3 --no-sign-request cp s3://human-pangenomics/T2T/CHM13/10x/CHM13_prep5_S13_L002_R2_001.fastq.gz .

While download happens, make a note of the download speed.

4. Comparing download speeds

Try downloading the Illumina reads using wget instead of S3:

    wget https://s3-us-west-2.amazonaws.com/human-pangenomics/T2T/CHM13/assemblies/chm13.draft_v1.1.fasta.gz
    
What do you notice?

5. Getting aligned reads

To see what the next bioinformatics steps would be, let us get BWA MEM and (attempt to) align the reads. 

The AWS instance is very much barebone, it is a fresh CentOS Linux with almost nothing preinstalled. Which means it doesn't have any bioinformatics tools, and it doesn't even have any compilation environment! (Try typing ``gcc``` for example.) For now we can work around it using a precompiled binary from conda (without installing conda):

    wget https://anaconda.org/bioconda/bwa/0.7.17/download/linux-64/bwa-0.7.17-h7132678_9.tar.bz2
    tar xf bwa-0.7.17-h7132678_9.tar.bz2
    
But we cannot align immediately, we need to index the genome first.

    bin/bwa index chm13.draft_v1.1.fasta.gz
    
Already this step takes much time (a few dozens of minutes at least). We could skip ahead and download a pre-made BAM alignment directly, like this:

    aws s3 --no-sign-request cp s3://human-pangenomics/T2T/CHM13/assemblies/alignments/chm13.draft_v1.1.pcrfree.bam .

.. But download will fail because the file is 250 GB large and we only have allocated a 30 GB disk space for our instance, 23 GB of which is used by the reads we just download. This is a valuable lession that properly dimensioning your cloud instance is critical! Yet, there are many ways to fix this (which we won't see here), such as redimensioning volumes and adding extra volumes.

We will not pursue further bioinformatics analyses on this particular instance, but hopefully you get the idea!

6. Bonus: getting data from SRA

Before we shut down this instance, let us try downloading reads from the SRA. Note that it is a 'us-west-2' instance and SRA data is located at 'us-east-1', so download speeds are not optimal. Yet, in my tests, they remain reasonable.

To get SRA data, we will use the excellent ffq tool that tells the location of the data. Fortunately, the instance at least has Python 3:

    pip3 install ffq
   
Let us say we want to download the PacBio HiFi data that was used to generate the CHM13 assembly. The HiFi reads are only available from the SRA, here: https://www.ncbi.nlm.nih.gov/sra/?term=SRX789768*+CHM13

Let's get the first accession:

    ffq --aws SRX7897688
   
You should see:

![image](https://user-images.githubusercontent.com/1218301/188704932-da88929f-d672-424a-a064-d76ba2e28a2b.png)


Unfortuantely, the first link given by ffq appears to point to a private bucket (I get a 'Forbidden' error when trying to download) . Yet, the second link works. Note that you can always transform a ```http://``` to a S3 url by extracting the bucket name from the URL.

    aws s3 cp aws s3  --no-sign-request cp s3://sra-pub-run-odp/sra/SRR11292120/SRR11292120 .

It should be that the download is still almost as fast as from a us-west-2 bucket. This is a clear advantage compared to download data to e.g. a university cluster. At this point, refer to [Part D](https://github.com/rchikhi/2022-pangenome-aws/blob/main/PartD.md) to delete the instance. If there is more time, you may go attempt Part C.
