## Part B: Downloading genomics data from AWS S3 to an AWS instance

1. Data description

We will download several types of data:
- a human reference genome
- a set of Illumina reads
- the same set of reads but pre-aligned

To illustrate that download speeds are really fast on AWS, we chose to create our instance on the us-west-2 region, which happens to be the same region where the T2T human genome is hosted (https://github.com/marbl/CHM13).

So, specifically, we will retrieve:
- the CHM13v2 genome assembly v1.1
- Illumina PCR-free reads generated by T2T from CHM13
- the alignment BAM file of those reads using BWA mem

Why CHM13v1.1 and not v2.0, you ask? It's because at the time of writing, the alignment BAM files and many other analyses are available for v1.1 but not v2.

2. S3

S3 is the name of the storage service/protocol used by Amazon. Amazon uses a lot of terminology for its services. A S3 'repository' is called a bucket. 

You can think of a S3 bucket as someone's Dropbox with a special command-line tool to access it. Some buckets are public, some are private, and you can imagine in both cases that permissions are set so that buckets can be either read-only or read-write by everyone. Of course, the T2T buckets are read-only for us. 

A particular aspect of S3 is that you may sometimes need a AWS account to download data if it is non-public. Yet, by default the S3 command-line tool always wants you to sign in to your account. We'll avoid that by passing the ```--no-sign-request``` argument.

3. Downloading the data

We'll get the data from https://github.com/marbl/CHM13.

Go to your instance and create a folder.

    mkdir data && cd data

Check that you have enough space:

    df -h .
    
Which should print:

    Filesystem      Size  Used Avail Use% Mounted on
    /dev/nvme0n1p1   30G  1,6G   29G   6% /

Download CHM13 v1.1:

    aws s3 --no-sign-request cp s3://human-pangenomics/T2T/CHM13/assemblies/chm13.draft_v1.1.fasta.gz .

Download (a portion of) PCRfree reads:

      aws s3 --no-sign-request cp s3://human-pangenomics/T2T/CHM13/10x/CHM13_prep5_S13_L002_R1_001.fastq.gz .
      aws s3 --no-sign-request cp s3://human-pangenomics/T2T/CHM13/10x/CHM13_prep5_S13_L002_R2_001.fastq.gz .

While download happens, make a note of the download speed.

4. Comparing download speeds

Try downloading the Illumina reads using wget instead of S3:

    wget https://s3-us-west-2.amazonaws.com/human-pangenomics/T2T/CHM13/assemblies/chm13.draft_v1.1.fasta.gz
    
What do you notice?

5. Getting aligned reads

To see what the next bioinformatics steps would be, let us get BWA MEM and (attempt to) align the reads. 

The AWS instance is very much barebone, it is a fresh CentOS Linux with almost nothing preinstalled. Which means it doesn't have any bioinformatics tools, and it doesn't even have any compilation environment! (Try typing ``gcc``` for example.) For now we can work around it using a precompiled binary from conda (without installing conda):

    wget https://anaconda.org/bioconda/bwa/0.7.17/download/linux-64/bwa-0.7.17-h7132678_9.tar.bz2
    tar xf bwa-0.7.17-h7132678_9.tar.bz2
    
But we cannot align immediately, we need to index the genome first.

    bin/bwa index chm13.draft_v1.1.fasta.gz
    
Already this step takes much time (a few dozens of minutes at least), so we'll skip ahead and download the alignment directoy:

    aws s3 --no-sign-request cp s3://human-pangenomics/T2T/CHM13/assemblies/alignments/chm13.draft_v1.1.pcrfree.bam .

Note however that this download will fail because the file is 250 GB large and we only have allocated a 30 GB disk space for our instance, 23 GB of which is used by the reads we just download. This is a valuable lession that properly dimensioning your cloud instance is critical! Yet, there are many ways to fix this (which we won't see here), such as redimensioning volumes and adding extra volumes.

We will not pursue further bioinformatics analyses on this particular instance, but hopefully you get the idea!

6. Bonus: getting data from SRA

Before we shut down this instance, let us try downloading reads from the SRA. Note that it is a 'us-west-2' instance and SRA data is located at 'us-east-1', so download speeds are not optimal. Yet, in my tests, they remain reasonable.

To get SRA data, we will use the excellent ffq tool that tells the location of the data. Fortunately, the instance at least has Python 3:

   pip3 install ffq
   
Let us say we want to download the PacBio HiFi data that was used to generate the CHM13 assembly. The HiFi reads are only available from the SRA, here: https://www.ncbi.nlm.nih.gov/sra/?term=SRX789768*+CHM13

Let's get the first accession:

   ffq --aws SRX7897688
   
You should see:

![image](https://user-images.githubusercontent.com/1218301/188704932-da88929f-d672-424a-a064-d76ba2e28a2b.png)


Unfortuantely, the first link given by ffq appears to be private. Yet, the second link works. Note that you can always transform a ```http://``` to a S3 url by extracting the bucket name from the URL.

    aws s3 cp aws s3  --no-sign-request cp s3://sra-pub-run-odp/sra/SRR11292120/SRR11292120 .
